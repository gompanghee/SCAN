{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCAN PyTorch Example Training\n",
    "\n",
    "This notebook demonstrates how to use the SCAN library with PyTorch.\n",
    "\n",
    "SCAN (Self-Confidence Attention Network) is a library designed to extract visual explanations from deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Make sure you have the following packages installed:\n",
    "- torch\n",
    "- torchvision\n",
    "- numpy\n",
    "- matplotlib\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install torch torchvision numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from SCAN import SCAN\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Target Model\n",
    "\n",
    "We'll use a pretrained ResNet50 model from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Number of parameters: 25,557,032\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet50\n",
    "target_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "target_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in target_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Preprocessing Function\n",
    "\n",
    "The preprocessing function should match the one used for the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization\n",
    "def preprocess_input(x):\n",
    "    \"\"\"\n",
    "    Preprocess input images for ResNet.\n",
    "    Input: torch.Tensor with values in [0, 255], shape (N, C, H, W)\n",
    "    Output: Normalized tensor\n",
    "    \"\"\"\n",
    "    # Normalize to [0, 1]\n",
    "    x = x / 255.0\n",
    "    \n",
    "    # ImageNet mean and std\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1, 3, 1, 1)\n",
    "    \n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset\n",
    "\n",
    "We'll use the real ImageNet dataset for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 1000\n",
      "First 5 synsets: ['n01440764', 'n01443537', 'n01484850', 'n01491361', 'n01494475']\n",
      "Sample image loaded from: /root/jupyter/SCAN2/ImageNet/train/n01440764/n01440764_10026.JPEG\n",
      "Image shape: torch.Size([3, 224, 224]), min: 0.0, max: 255.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# ImageNet dataset paths\n",
    "IMAGENET_PATH = '/root/jupyter/SCAN2/ImageNet'\n",
    "TRAIN_PATH = os.path.join(IMAGENET_PATH, 'train')\n",
    "VAL_PATH = os.path.join(IMAGENET_PATH, 'val')\n",
    "VAL_ANN_PATH = os.path.join(IMAGENET_PATH, 'Annotations/CLS-LOC/val')\n",
    "\n",
    "# Build synset to index mapping from train folder (alphabetical order)\n",
    "synset_list = sorted(os.listdir(TRAIN_PATH))\n",
    "synset_to_idx = {synset: idx for idx, synset in enumerate(synset_list)}\n",
    "print(f\"Number of classes: {len(synset_to_idx)}\")\n",
    "print(f\"First 5 synsets: {synset_list[:5]}\")\n",
    "\n",
    "class ImageNetTrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ImageNet training dataset wrapper that returns images in [0, 255] range.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, image_size=(224, 224), max_samples=None):\n",
    "        self.image_size = image_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(image_size[0]),\n",
    "            transforms.ToTensor(),  # Converts to [0, 1]\n",
    "        ])\n",
    "        self.dataset = ImageFolder(root, transform=self.transform)\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.max_samples:\n",
    "            return min(len(self.dataset), self.max_samples)\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        # Convert from [0, 1] to [0, 255]\n",
    "        image = image * 255.0\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class ImageNetValDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ImageNet validation dataset with correct labels from XML annotations.\n",
    "    Returns images in [0, 255] range.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, ann_dir, synset_to_idx, image_size=(224, 224), max_samples=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.synset_to_idx = synset_to_idx\n",
    "        self.samples = []\n",
    "        \n",
    "        # Get all validation images\n",
    "        img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.JPEG')])\n",
    "        \n",
    "        for img_file in img_files:\n",
    "            # Get synset from XML annotation\n",
    "            xml_file = img_file.replace('.JPEG', '.xml')\n",
    "            xml_path = os.path.join(ann_dir, xml_file)\n",
    "            \n",
    "            if os.path.exists(xml_path):\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "                synset = root.find('.//object/name').text\n",
    "                \n",
    "                if synset in synset_to_idx:\n",
    "                    label = synset_to_idx[synset]\n",
    "                    self.samples.append((img_file, label))\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(image_size[0]),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        self.max_samples = max_samples\n",
    "        print(f\"Loaded {len(self.samples)} validation samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.max_samples:\n",
    "            return min(len(self.samples), self.max_samples)\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name, label = self.samples[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = self.transform(img) * 255.0  # Convert to [0, 255]\n",
    "        return img_tensor, label\n",
    "\n",
    "\n",
    "def load_sample_image(image_path=None):\n",
    "    \"\"\"Load a sample image from ImageNet for testing.\"\"\"\n",
    "    if image_path is None:\n",
    "        # Get a random image from the first class\n",
    "        first_class = sorted(os.listdir(TRAIN_PATH))[0]\n",
    "        class_path = os.path.join(TRAIN_PATH, first_class)\n",
    "        image_name = sorted(os.listdir(class_path))[0]\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "    \n",
    "    # Load and transform image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img_tensor = transform(img) * 255.0  # Convert to [0, 255]\n",
    "    return img_tensor, image_path\n",
    "\n",
    "\n",
    "# Test loading a sample image\n",
    "test_img, test_path = load_sample_image()\n",
    "print(f\"Sample image loaded from: {test_path}\")\n",
    "print(f\"Image shape: {test_img.shape}, min: {test_img.min():.1f}, max: {test_img.max():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 validation samples\n",
      "Training samples: 1281167\n",
      "Validation samples: 50000\n",
      "Batch size: 128\n",
      "Training batches: 10010\n",
      "Validation batches: 391\n"
     ]
    }
   ],
   "source": [
    "# Create ImageNet train and validation datasets\n",
    "# For quick testing, limit the number of samples (set to None for full dataset)\n",
    "MAX_TRAIN_SAMPLES = None   # Use None for full dataset (~1.2M images)\n",
    "MAX_VAL_SAMPLES = None     # Use None for full validation (50K images)\n",
    "\n",
    "# Training dataset from train folder\n",
    "train_dataset = ImageNetTrainDataset(TRAIN_PATH, image_size=(224, 224), max_samples=MAX_TRAIN_SAMPLES)\n",
    "\n",
    "# Validation dataset from val folder with correct labels from XML annotations\n",
    "valid_dataset = ImageNetValDataset(\n",
    "    img_dir=VAL_PATH,\n",
    "    ann_dir=VAL_ANN_PATH,\n",
    "    synset_to_idx=synset_to_idx,\n",
    "    image_size=(224, 224),\n",
    "    max_samples=MAX_VAL_SAMPLES\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 128  # Increased from 16 for better training efficiency\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Find Target Layer\n",
    "\n",
    "Let's explore the model architecture to find the appropriate target layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available layers in ResNet50:\n",
      "==================================================\n",
      "conv1: Conv2d\n",
      "bn1: BatchNorm2d\n",
      "relu: ReLU\n",
      "maxpool: MaxPool2d\n",
      "layer1: Sequential\n",
      "layer1.0: Bottleneck\n",
      "layer1.0.conv1: Conv2d\n",
      "layer1.0.bn1: BatchNorm2d\n",
      "layer1.0.conv2: Conv2d\n",
      "layer1.0.bn2: BatchNorm2d\n",
      "layer1.0.conv3: Conv2d\n",
      "layer1.0.bn3: BatchNorm2d\n",
      "layer1.0.relu: ReLU\n",
      "layer1.0.downsample: Sequential\n",
      "layer1.0.downsample.0: Conv2d\n",
      "layer1.0.downsample.1: BatchNorm2d\n",
      "layer1.1: Bottleneck\n",
      "layer1.1.conv1: Conv2d\n",
      "layer1.1.bn1: BatchNorm2d\n",
      "layer1.1.conv2: Conv2d\n",
      "layer1.1.bn2: BatchNorm2d\n",
      "layer1.1.conv3: Conv2d\n",
      "layer1.1.bn3: BatchNorm2d\n",
      "layer1.1.relu: ReLU\n",
      "layer1.2: Bottleneck\n",
      "layer1.2.conv1: Conv2d\n",
      "layer1.2.bn1: BatchNorm2d\n",
      "layer1.2.conv2: Conv2d\n",
      "layer1.2.bn2: BatchNorm2d\n",
      "layer1.2.conv3: Conv2d\n",
      "layer1.2.bn3: BatchNorm2d\n",
      "layer1.2.relu: ReLU\n",
      "layer2: Sequential\n",
      "layer2.0: Bottleneck\n",
      "layer2.0.conv1: Conv2d\n",
      "layer2.0.bn1: BatchNorm2d\n",
      "layer2.0.conv2: Conv2d\n",
      "layer2.0.bn2: BatchNorm2d\n",
      "layer2.0.conv3: Conv2d\n",
      "layer2.0.bn3: BatchNorm2d\n",
      "layer2.0.relu: ReLU\n",
      "layer2.0.downsample: Sequential\n",
      "layer2.0.downsample.0: Conv2d\n",
      "layer2.0.downsample.1: BatchNorm2d\n",
      "layer2.1: Bottleneck\n",
      "layer2.1.conv1: Conv2d\n",
      "layer2.1.bn1: BatchNorm2d\n",
      "layer2.1.conv2: Conv2d\n",
      "layer2.1.bn2: BatchNorm2d\n",
      "layer2.1.conv3: Conv2d\n",
      "layer2.1.bn3: BatchNorm2d\n",
      "layer2.1.relu: ReLU\n",
      "layer2.2: Bottleneck\n",
      "layer2.2.conv1: Conv2d\n",
      "layer2.2.bn1: BatchNorm2d\n",
      "layer2.2.conv2: Conv2d\n",
      "layer2.2.bn2: BatchNorm2d\n",
      "layer2.2.conv3: Conv2d\n",
      "layer2.2.bn3: BatchNorm2d\n",
      "layer2.2.relu: ReLU\n",
      "layer2.3: Bottleneck\n",
      "layer2.3.conv1: Conv2d\n",
      "layer2.3.bn1: BatchNorm2d\n",
      "layer2.3.conv2: Conv2d\n",
      "layer2.3.bn2: BatchNorm2d\n",
      "layer2.3.conv3: Conv2d\n",
      "layer2.3.bn3: BatchNorm2d\n",
      "layer2.3.relu: ReLU\n",
      "layer3: Sequential\n",
      "layer3.0: Bottleneck\n",
      "layer3.0.conv1: Conv2d\n",
      "layer3.0.bn1: BatchNorm2d\n",
      "layer3.0.conv2: Conv2d\n",
      "layer3.0.bn2: BatchNorm2d\n",
      "layer3.0.conv3: Conv2d\n",
      "layer3.0.bn3: BatchNorm2d\n",
      "layer3.0.relu: ReLU\n",
      "layer3.0.downsample: Sequential\n",
      "layer3.0.downsample.0: Conv2d\n",
      "layer3.0.downsample.1: BatchNorm2d\n",
      "layer3.1: Bottleneck\n",
      "layer3.1.conv1: Conv2d\n",
      "layer3.1.bn1: BatchNorm2d\n",
      "layer3.1.conv2: Conv2d\n",
      "layer3.1.bn2: BatchNorm2d\n",
      "layer3.1.conv3: Conv2d\n",
      "layer3.1.bn3: BatchNorm2d\n",
      "layer3.1.relu: ReLU\n",
      "layer3.2: Bottleneck\n",
      "layer3.2.conv1: Conv2d\n",
      "layer3.2.bn1: BatchNorm2d\n",
      "layer3.2.conv2: Conv2d\n",
      "layer3.2.bn2: BatchNorm2d\n",
      "layer3.2.conv3: Conv2d\n",
      "layer3.2.bn3: BatchNorm2d\n",
      "layer3.2.relu: ReLU\n",
      "layer3.3: Bottleneck\n",
      "layer3.3.conv1: Conv2d\n",
      "layer3.3.bn1: BatchNorm2d\n",
      "layer3.3.conv2: Conv2d\n",
      "layer3.3.bn2: BatchNorm2d\n",
      "layer3.3.conv3: Conv2d\n",
      "layer3.3.bn3: BatchNorm2d\n",
      "layer3.3.relu: ReLU\n",
      "layer3.4: Bottleneck\n",
      "layer3.4.conv1: Conv2d\n",
      "layer3.4.bn1: BatchNorm2d\n",
      "layer3.4.conv2: Conv2d\n",
      "layer3.4.bn2: BatchNorm2d\n",
      "layer3.4.conv3: Conv2d\n",
      "layer3.4.bn3: BatchNorm2d\n",
      "layer3.4.relu: ReLU\n",
      "layer3.5: Bottleneck\n",
      "layer3.5.conv1: Conv2d\n",
      "layer3.5.bn1: BatchNorm2d\n",
      "layer3.5.conv2: Conv2d\n",
      "layer3.5.bn2: BatchNorm2d\n",
      "layer3.5.conv3: Conv2d\n",
      "layer3.5.bn3: BatchNorm2d\n",
      "layer3.5.relu: ReLU\n",
      "layer4: Sequential\n",
      "layer4.0: Bottleneck\n",
      "layer4.0.conv1: Conv2d\n",
      "layer4.0.bn1: BatchNorm2d\n",
      "layer4.0.conv2: Conv2d\n",
      "layer4.0.bn2: BatchNorm2d\n",
      "layer4.0.conv3: Conv2d\n",
      "layer4.0.bn3: BatchNorm2d\n",
      "layer4.0.relu: ReLU\n",
      "layer4.0.downsample: Sequential\n",
      "layer4.0.downsample.0: Conv2d\n",
      "layer4.0.downsample.1: BatchNorm2d\n",
      "layer4.1: Bottleneck\n",
      "layer4.1.conv1: Conv2d\n",
      "layer4.1.bn1: BatchNorm2d\n",
      "layer4.1.conv2: Conv2d\n",
      "layer4.1.bn2: BatchNorm2d\n",
      "layer4.1.conv3: Conv2d\n",
      "layer4.1.bn3: BatchNorm2d\n",
      "layer4.1.relu: ReLU\n",
      "layer4.2: Bottleneck\n",
      "layer4.2.conv1: Conv2d\n",
      "layer4.2.bn1: BatchNorm2d\n",
      "layer4.2.conv2: Conv2d\n",
      "layer4.2.bn2: BatchNorm2d\n",
      "layer4.2.conv3: Conv2d\n",
      "layer4.2.bn3: BatchNorm2d\n",
      "layer4.2.relu: ReLU\n",
      "avgpool: AdaptiveAvgPool2d\n",
      "fc: Linear\n"
     ]
    }
   ],
   "source": [
    "# Print model layer names\n",
    "print(\"Available layers in ResNet50:\")\n",
    "print(\"=\" * 50)\n",
    "for name, module in target_model.named_modules():\n",
    "    if name:  # Skip empty name (the model itself)\n",
    "        print(f\"{name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target layer: layer4\n"
     ]
    }
   ],
   "source": [
    "# Common target layers for ResNet50:\n",
    "# - 'layer4': Last convolutional block (recommended)\n",
    "# - 'layer4.2.conv3': Last conv layer before avgpool\n",
    "# - 'layer3': Earlier features (more detailed but noisier)\n",
    "\n",
    "target_layer_name = 'layer4'  # Recommended for most use cases\n",
    "print(f\"Using target layer: {target_layer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize SCAN\n",
    "\n",
    "Create the SCAN object with the target model and layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCAN initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize SCAN\n",
    "scanner = SCAN(\n",
    "    target_model=target_model,\n",
    "    target_layer=target_layer_name,\n",
    "    image_size=(224, 224),\n",
    "    use_gradient_mask=True,\n",
    "    device=device,\n",
    "    num_classes=1000  # ImageNet classes\n",
    ")\n",
    "\n",
    "# Set preprocessing function\n",
    "scanner.set_preprocess(preprocess_input)\n",
    "\n",
    "print(\"SCAN initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Training\n",
    "\n",
    "Configure the decoder model, optimizer, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete!\n",
      "Decoder parameters: 8,257,668\n",
      "Batch size: 128\n",
      "Learning rate: 0.001 with Cosine Annealing (eta_min=1e-6)\n",
      "Epochs: 2\n"
     ]
    }
   ],
   "source": [
    "# Set datasets (use_augmentation=(70, 100) is the paper default for training)\n",
    "scanner.set_dataset(train_loader, use_augmentation=(70, 100))\n",
    "scanner.set_validation_dataset(valid_loader)\n",
    "\n",
    "# Generate decoder (convolutional decoder for CNN features)\n",
    "scanner.generate_decoder(is_Transformer=False)\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Compile with optimizer, loss, and Cosine Annealing LR scheduler\n",
    "scanner.compile(\n",
    "    loss_alpha=4.0,\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    scheduler_class=torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "    scheduler_kwargs={'T_max': EPOCHS, 'eta_min': 1e-6}\n",
    ")\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Decoder parameters: {sum(p.numel() for p in scanner.decoder.parameters()):,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE} with Cosine Annealing (eta_min=1e-6)\")\n",
    "print(f\"Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train SCAN Decoder\n",
    "\n",
    "Train the decoder to learn visual explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, clear_output, Image as IPImage, HTML\n",
    "from io import BytesIO\n",
    "\n",
    "# Create directory for saving visualization snapshots\n",
    "VIS_DIR = './training_snapshots'\n",
    "os.makedirs(VIS_DIR, exist_ok=True)\n",
    "\n",
    "def create_validation_figure(scanner, valid_loader, num_samples=4, percentile=95, save_path=None):\n",
    "    \"\"\"Create and save a figure with SCAN results on random validation samples.\"\"\"\n",
    "    scanner.decoder.eval()\n",
    "    \n",
    "    # Get random validation samples\n",
    "    indices = random.sample(range(len(valid_loader.dataset)), num_samples)\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx in indices:\n",
    "        img, label = valid_loader.dataset[idx]\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    batch_images = torch.stack(images).to(scanner.device)\n",
    "    \n",
    "    # Generate SCAN explanations\n",
    "    with torch.no_grad():\n",
    "        confidence_maps, reconstructed_images = scanner(batch_images, percentile=percentile)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        original = batch_images[i].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "        axes[i, 0].imshow(original)\n",
    "        axes[i, 0].set_title(f'Original (class: {labels[i]})')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Reconstructed image\n",
    "        axes[i, 1].imshow(reconstructed_images[i].cpu().numpy())\n",
    "        axes[i, 1].set_title('Reconstructed')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Confidence map\n",
    "        conf_map = confidence_maps[i].cpu().numpy()\n",
    "        im = axes[i, 2].imshow(conf_map, cmap='jet', vmin=0, vmax=1)\n",
    "        axes[i, 2].set_title(f'Confidence (min:{conf_map.min():.2f}, max:{conf_map.max():.2f})')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        axes[i, 3].imshow(original)\n",
    "        axes[i, 3].imshow(conf_map, cmap='jet', alpha=0.5, vmin=0, vmax=1)\n",
    "        axes[i, 3].set_title('Overlay')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        return save_path\n",
    "    else:\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "        return None\n",
    "\n",
    "\n",
    "def train_with_visualization(scanner, train_loader, valid_loader, epochs=2, \n",
    "                             visualize_every=1000, num_vis_samples=4):\n",
    "    \"\"\"\n",
    "    Custom training loop with periodic visualization of validation samples.\n",
    "    Saves snapshots to files and displays the latest one.\n",
    "    \n",
    "    Args:\n",
    "        scanner: SCAN instance\n",
    "        train_loader: Training DataLoader\n",
    "        valid_loader: Validation DataLoader  \n",
    "        epochs: Number of training epochs\n",
    "        visualize_every: Visualize every N batches\n",
    "        num_vis_samples: Number of samples to visualize\n",
    "    \n",
    "    Returns:\n",
    "        history: Training history dict\n",
    "        snapshot_paths: List of saved snapshot file paths\n",
    "    \"\"\"\n",
    "    history = {'loss': [], 'val_loss': [], 'ConfMAE_Metric': [], 'NoConfMAE_Metric': [], 'lr': []}\n",
    "    snapshot_paths = []\n",
    "    \n",
    "    print(f\"Training with visualization every {visualize_every} batches\")\n",
    "    print(f\"Snapshots will be saved to: {VIS_DIR}/\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    global_batch = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        scanner.decoder.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Reset metrics\n",
    "        for metric in scanner.metrics:\n",
    "            metric.reset()\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = scanner.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} (LR: {current_lr:.2e})')\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(scanner.device).float()\n",
    "            labels = labels.to(scanner.device)\n",
    "            \n",
    "            # Use SCAN's internal _process_batch method with labels for correct gradient computation\n",
    "            feature_maps, target_images = scanner._process_batch(images, labels=labels, use_augmentation=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            scanner.optimizer.zero_grad()\n",
    "            outputs = scanner.decoder(feature_maps)\n",
    "            loss = scanner.criterion(outputs, target_images)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            scanner.optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            global_batch += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for metric in scanner.metrics:\n",
    "                    metric.update(target_images, outputs)\n",
    "\n",
    "            # Build postfix with loss, metrics, and lr\n",
    "            postfix = {'loss': f'{loss.item():.4f}'}\n",
    "            for metric in scanner.metrics:\n",
    "                metric_name = metric.__class__.__name__\n",
    "                display_name = 'CMAE' if 'ConfMAE' in metric_name and 'NoConf' not in metric_name else 'NCMAE' if 'NoConf' in metric_name else metric_name\n",
    "                postfix[display_name] = f'{metric.compute():.4f}'\n",
    "            postfix['lr'] = f'{current_lr:.2e}'\n",
    "            pbar.set_postfix(postfix)\n",
    "            \n",
    "            # Visualize every N batches\n",
    "            if global_batch % visualize_every == 0:\n",
    "                save_path = os.path.join(VIS_DIR, f'snapshot_batch_{global_batch:06d}.png')\n",
    "                create_validation_figure(scanner, valid_loader, \n",
    "                                        num_samples=num_vis_samples,\n",
    "                                        save_path=save_path)\n",
    "                snapshot_paths.append(save_path)\n",
    "                print(f\"\\n[Batch {global_batch}] Snapshot saved: {save_path}\")\n",
    "                \n",
    "                # Display the latest snapshot\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(f\"<h3>Latest Snapshot (Batch {global_batch})</h3>\"))\n",
    "                display(IPImage(filename=save_path))\n",
    "                \n",
    "                # Re-create progress bar after clear_output\n",
    "                pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} (LR: {current_lr:.2e})', \n",
    "                           initial=batch_idx+1, total=len(train_loader))\n",
    "                scanner.decoder.train()\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_loss = epoch_loss / max(num_batches, 1)\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        for metric in scanner.metrics:\n",
    "            history[metric.__class__.__name__].append(metric.compute())\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\", end=\"\")\n",
    "        for metric in scanner.metrics:\n",
    "            print(f\" - {metric.__class__.__name__}: {history[metric.__class__.__name__][-1]:.4f}\", end=\"\")\n",
    "        print(f\" - LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # Validation at end of epoch\n",
    "        if valid_loader is not None:\n",
    "            print(\"Running validation...\")\n",
    "            val_loss, val_metrics = scanner._validate()\n",
    "            history['val_loss'].append(val_loss)\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Step the learning rate scheduler after each epoch\n",
    "        if scanner.scheduler is not None:\n",
    "            scanner.scheduler.step()\n",
    "        \n",
    "        scanner.decoder.train()\n",
    "    \n",
    "    print(f\"\\nTraining complete! {len(snapshot_paths)} snapshots saved to {VIS_DIR}/\")\n",
    "    return history, snapshot_paths\n",
    "\n",
    "\n",
    "# Train with visualization every 1000 batches\n",
    "history, snapshot_paths = train_with_visualization(\n",
    "    scanner, \n",
    "    train_loader, \n",
    "    valid_loader, \n",
    "    epochs=EPOCHS, \n",
    "    visualize_every=1000,\n",
    "    num_vis_samples=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['loss'], label='Train Loss')\n",
    "if history['val_loss']:\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Metrics plot\n",
    "if 'ConfMAE_Metric' in history:\n",
    "    axes[1].plot(history['ConfMAE_Metric'], label='Confident MAE')\n",
    "if 'NoConfMAE_Metric' in history:\n",
    "    axes[1].plot(history['NoConfMAE_Metric'], label='Not Confident MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Training Metrics')\n",
    "axes[1].legend()\n",
    "\n",
    "# Learning rate plot\n",
    "if 'lr' in history and history['lr']:\n",
    "    axes[2].plot(history['lr'], label='Learning Rate', color='green')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('Learning Rate Schedule (Cosine Annealing)')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Note: You can revisit training visualizations anytime using:\n",
    "# vis_carousel.show()\n",
    "print(f\"\\nTotal visualization snapshots saved: {len(vis_carousel.snapshots)}\")\n",
    "print(\"Use 'vis_carousel.show()' to browse through training snapshots again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Visual Explanations\n",
    "\n",
    "Use the trained SCAN to generate visual explanations for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample images from validation set for testing\n",
    "import random\n",
    "\n",
    "def load_random_val_images(valid_dataset, num_images=5):\n",
    "    \"\"\"Load random images from ImageNet validation set.\"\"\"\n",
    "    indices = random.sample(range(len(valid_dataset)), min(num_images, len(valid_dataset)))\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx in indices:\n",
    "        img, label = valid_dataset[idx]\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Load test images from validation set\n",
    "test_images, test_labels = load_random_val_images(valid_dataset, 5)\n",
    "test_image = test_images[0]\n",
    "print(f\"Loaded {len(test_images)} test images from validation set\")\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Test labels: {test_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visual explanation\n",
    "confidence_map, reconstructed_image = scanner(test_image, percentile=95)\n",
    "\n",
    "print(f\"Confidence map shape: {confidence_map.shape}\")\n",
    "print(f\"Reconstructed image shape: {reconstructed_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(test_image.permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Reconstructed image\n",
    "axes[1].imshow(reconstructed_image.numpy())\n",
    "axes[1].set_title('Reconstructed Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Confidence map\n",
    "im = axes[2].imshow(confidence_map.numpy(), cmap='jet')\n",
    "axes[2].set_title('Confidence Map')\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay confidence map on original image\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "# Show original image\n",
    "original = test_image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "ax.imshow(original)\n",
    "\n",
    "# Overlay confidence map\n",
    "conf_map = confidence_map.numpy()\n",
    "ax.imshow(conf_map, cmap='jet', alpha=0.5)\n",
    "\n",
    "ax.set_title('Visual Explanation Overlay')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save and Load Decoder\n",
    "\n",
    "Save the trained decoder for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save decoder\n",
    "scanner.save_decoder('scan_decoder_resnet50.pt')\n",
    "print(\"Decoder saved to 'scan_decoder_resnet50.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decoder (for demonstration)\n",
    "# scanner.load_decoder('scan_decoder_resnet50.pt')\n",
    "# print(\"Decoder loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Batch Processing\n",
    "\n",
    "Generate explanations for multiple images at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch of images from ImageNet\n",
    "batch_images = torch.stack(test_images)\n",
    "print(f\"Batch shape: {batch_images.shape}\")\n",
    "\n",
    "# Generate explanations for batch\n",
    "confidence_maps, reconstructed_images = scanner(batch_images, percentile=95)\n",
    "\n",
    "print(f\"Confidence maps shape: {confidence_maps.shape}\")\n",
    "print(f\"Reconstructed images shape: {reconstructed_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch results\n",
    "num_images = len(test_images)\n",
    "fig, axes = plt.subplots(num_images, 3, figsize=(12, 4 * num_images))\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Original\n",
    "    axes[i, 0].imshow(batch_images[i].permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "    axes[i, 0].set_title(f'Original {i+1}')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[i, 1].imshow(reconstructed_images[i].numpy())\n",
    "    axes[i, 1].set_title(f'Reconstructed {i+1}')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Confidence map\n",
    "    axes[i, 2].imshow(confidence_maps[i].numpy(), cmap='jet')\n",
    "    axes[i, 2].set_title(f'Confidence {i+1}')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Dataset Configuration\n",
    "This notebook uses the ImageNet dataset:\n",
    "- **Training**: `/root/jupyter/SCAN2/ImageNet/train` (ImageFolder with synset-based labels)\n",
    "- **Validation**: `/root/jupyter/SCAN2/ImageNet/val` with XML annotations from `/root/jupyter/SCAN2/ImageNet/Annotations/CLS-LOC/val`\n",
    "\n",
    "The validation labels are extracted from XML annotation files to ensure correct class mapping (alphabetical synset order).\n",
    "\n",
    "To adjust the amount of data used:\n",
    "```python\n",
    "MAX_TRAIN_SAMPLES = 10000  # Set to None for full dataset (~1.2M images)\n",
    "MAX_VAL_SAMPLES = 5000     # Set to None for full validation (~50K images)\n",
    "BATCH_SIZE = 128           # Default: 128 (increase if GPU memory allows)\n",
    "```\n",
    "\n",
    "### Training Configuration\n",
    "Current settings:\n",
    "- **Batch Size**: 128\n",
    "- **Learning Rate**: 1e-3 with Cosine Annealing scheduler\n",
    "- **LR Scheduler**: CosineAnnealingLR (eta_min=1e-6)\n",
    "- **Epochs**: 2 (increase to 5-10 for better results)\n",
    "\n",
    "```python\n",
    "scanner.compile(\n",
    "    loss_alpha=4.0,\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    scheduler_class=torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "    scheduler_kwargs={'T_max': EPOCHS, 'eta_min': 1e-6}\n",
    ")\n",
    "```\n",
    "\n",
    "### Visualization Carousel (iPyWidgets)\n",
    "The training loop now includes an interactive carousel widget for browsing visualizations:\n",
    "- **◀ Previous / Next ▶** buttons to navigate between snapshots\n",
    "- **Slider** for quick navigation to any step\n",
    "- **Auto-updates** during training with new snapshots\n",
    "\n",
    "```python\n",
    "history, vis_carousel = train_with_visualization(\n",
    "    scanner, \n",
    "    train_loader, \n",
    "    valid_loader, \n",
    "    epochs=2, \n",
    "    visualize_every=1000,  # Capture snapshot every 1000 batches\n",
    "    num_vis_samples=4      # Number of samples per snapshot\n",
    ")\n",
    "\n",
    "# Re-display carousel anytime after training:\n",
    "vis_carousel.show()\n",
    "```\n",
    "\n",
    "Each snapshot shows 4 columns:\n",
    "1. **Original**: Input image with class label\n",
    "2. **Reconstructed**: Decoder output (blurred target)\n",
    "3. **Confidence Map**: Self-confidence map (jet colormap)\n",
    "4. **Overlay**: Confidence map overlaid on original\n",
    "\n",
    "### For Better Results:\n",
    "1. **Full Dataset**: Set `MAX_TRAIN_SAMPLES = None` to use the entire ImageNet training set\n",
    "2. **More Epochs**: Train for more epochs (5-10 recommended)\n",
    "3. **Larger Batch Size**: Use larger batches if GPU memory allows (256, 512)\n",
    "4. **Learning Rate Schedule**: Cosine Annealing is already applied\n",
    "\n",
    "### Choosing Target Layer:\n",
    "- **layer4**: Best for high-level semantic features (recommended)\n",
    "- **layer3**: More detailed but noisier\n",
    "- **layer2**: Very detailed, may be too noisy\n",
    "\n",
    "### For Transformer Models:\n",
    "If using Vision Transformer (ViT) or similar:\n",
    "```python\n",
    "scanner.generate_decoder(is_Transformer=True)\n",
    "```\n",
    "\n",
    "### Memory Tips:\n",
    "- If you run out of GPU memory, reduce `BATCH_SIZE`\n",
    "- Use `NUM_WORKERS=0` if you encounter multiprocessing issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
